{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AyeChanAung205/-business-data/blob/main/MS_MFU_DataPreparation_Part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing and Cleaning\n",
        "1.Handling Missing Values\n",
        "\n",
        "> Missing data can bias results and reduce model performance.\n",
        "> Methods to handle missing values:\n",
        "*   Remove missing values (if they are too few).\n",
        "*   Impute values (mean, median, mode, or advanced techniques).\n",
        "\n"
      ],
      "metadata": {
        "id": "REQsAygHSgtk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODCpimm7SYjW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c90a4fa-efe3-4bea-f927-e4de8b38356c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data:\n",
            "     Age    Salary\n",
            "0  25.0   50000.0\n",
            "1  30.0   60000.0\n",
            "2   NaN   70000.0\n",
            "3  35.0   80000.0\n",
            "4  40.0       NaN\n",
            "5   NaN  100000.0\n",
            "\n",
            "Data after dropping missing values:\n",
            "     Age   Salary\n",
            "0  25.0  50000.0\n",
            "1  30.0  60000.0\n",
            "3  35.0  80000.0\n",
            "\n",
            "Data after filling missing values with mean:\n",
            "     Age    Salary\n",
            "0  25.0   50000.0\n",
            "1  30.0   60000.0\n",
            "2  32.5   70000.0\n",
            "3  35.0   80000.0\n",
            "4  40.0   72000.0\n",
            "5  32.5  100000.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Sample dataset with missing values\n",
        "data = {'Age': [25, 30, np.nan, 35, 40, np.nan],\n",
        "        'Salary': [50000, 60000, 70000, 80000, np.nan, 100000]}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(\"Original Data:\\n\", df)\n",
        "\n",
        "# Option 1: Remove rows with missing values\n",
        "df_dropped = df.dropna()\n",
        "print(\"\\nData after dropping missing values:\\n\", df_dropped)\n",
        "\n",
        "# Option 2: Fill missing values with mean\n",
        "df_filled = df.fillna(df.mean(numeric_only=True))\n",
        "print(\"\\nData after filling missing values with mean:\\n\", df_filled)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Handling Duplicate Records\n",
        "> Duplicate records can distort analysis and lead to incorrect conclusions.\n",
        "\n"
      ],
      "metadata": {
        "id": "ds8s2UdTWKiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample dataset with duplicate records\n",
        "df_dup = pd.DataFrame({'ID': [1, 2, 2, 3, 4, 4],\n",
        "                        'Name': ['Alice', 'Bob', 'Bob', 'Charlie', 'David', 'David'],\n",
        "                        'Salary': [50000, 60000, 60000, 70000, 80000, 80000]})\n",
        "\n",
        "print(\"Original Data:\\n\", df_dup)\n",
        "\n",
        "# Removing duplicate rows\n",
        "df_no_duplicates = df_dup.drop_duplicates()\n",
        "print(\"\\nData after removing duplicates:\\n\", df_no_duplicates)\n"
      ],
      "metadata": {
        "id": "1v4x1WKdWdLK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb8a4406-4b7d-4539-bed2-fb49f76b290e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data:\n",
            "    ID     Name  Salary\n",
            "0   1    Alice   50000\n",
            "1   2      Bob   60000\n",
            "2   2      Bob   60000\n",
            "3   3  Charlie   70000\n",
            "4   4    David   80000\n",
            "5   4    David   80000\n",
            "\n",
            "Data after removing duplicates:\n",
            "    ID     Name  Salary\n",
            "0   1    Alice   50000\n",
            "1   2      Bob   60000\n",
            "3   3  Charlie   70000\n",
            "4   4    David   80000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Handling Noisy Data\n",
        "> Data that contains errors, outliers, or inconsistencies.\n",
        "> Methods to handle noisy data:\n",
        "*   Smoothing techniques (moving averages, binning)\n",
        "*   Removing outliers using statistical methods (Z-score, IQR)"
      ],
      "metadata": {
        "id": "1TRblpw0YGAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generating synthetic data with outliers\n",
        "df_noisy = pd.DataFrame({'Value': [10, 12, 14, 13, 11, 120, 10, 9, 130, 12]})\n",
        "\n",
        "# Identifying outliers using Interquartile Range (IQR)\n",
        "Q1 = df_noisy['Value'].quantile(0.25)\n",
        "Q3 = df_noisy['Value'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "df_cleaned = df_noisy[(df_noisy['Value'] >= lower_bound) & (df_noisy['Value'] <= upper_bound)]\n",
        "print(\"Cleaned Data without outliers:\\n\", df_cleaned)\n",
        "\n",
        "# Plotting before and after\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.boxplot(y=df_noisy['Value'])\n",
        "plt.title(\"Before Cleaning\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.boxplot(y=df_cleaned['Value'])\n",
        "plt.title(\"After Removing Outliers\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tZi0BARinPV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.Converting Categorical Data to Numerical\n",
        "\n",
        "> Machine learning models require numerical input.\n",
        "> Methods:\n",
        "*   Label Encoding (for ordinal categories)\n",
        "*   One-Hot Encoding (for non-ordinal categories)"
      ],
      "metadata": {
        "id": "G6LmSTQ1qGay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "# Sample dataset with categorical values\n",
        "df_cat = pd.DataFrame({'City': ['New York', 'London', 'Paris', 'New York', 'Paris']})\n",
        "\n",
        "# Label Encoding\n",
        "label_encoder = LabelEncoder()\n",
        "df_cat['City_Label'] = label_encoder.fit_transform(df_cat['City'])\n",
        "\n",
        "# One-Hot Encoding\n",
        "df_onehot = pd.get_dummies(df_cat, columns=['City'])\n",
        "\n",
        "print(\"Label Encoding:\\n\", df_cat)\n",
        "print(\"\\nOne-Hot Encoding:\\n\", df_onehot)\n"
      ],
      "metadata": {
        "id": "rWH-sjNqqaHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.Normalization & Standardization\n",
        "*   Normalization (Min-Max Scaling): Rescales values between 0 and 1.\n",
        "*   Standardization (Z-Score Scaling): Centers data around mean 0 with standard deviation 1.\n"
      ],
      "metadata": {
        "id": "JTuybBh-rCuh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "# Sample dataset\n",
        "df_scale = pd.DataFrame({'Age': [25, 30, 35, 40, 45],\n",
        "                         'Salary': [50000, 60000, 70000, 80000, 90000]})\n",
        "\n",
        "# Min-Max Scaling\n",
        "scaler = MinMaxScaler()\n",
        "df_scaled = pd.DataFrame(scaler.fit_transform(df_scale), columns=df_scale.columns)\n",
        "\n",
        "# Standardization\n",
        "standard_scaler = StandardScaler()\n",
        "df_standardized = pd.DataFrame(standard_scaler.fit_transform(df_scale), columns=df_scale.columns)\n",
        "\n",
        "print(\"\\nNormalized Data (Min-Max Scaling):\\n\", df_scaled)\n",
        "print(\"\\nStandardized Data (Z-Score):\\n\", df_standardized)\n"
      ],
      "metadata": {
        "id": "iq4VTKeirMgX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7477b93-0813-401d-b619-c5dc010309b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Normalized Data (Min-Max Scaling):\n",
            "     Age  Salary\n",
            "0  0.00    0.00\n",
            "1  0.25    0.25\n",
            "2  0.50    0.50\n",
            "3  0.75    0.75\n",
            "4  1.00    1.00\n",
            "\n",
            "Standardized Data (Z-Score):\n",
            "         Age    Salary\n",
            "0 -1.414214 -1.414214\n",
            "1 -0.707107 -0.707107\n",
            "2  0.000000  0.000000\n",
            "3  0.707107  0.707107\n",
            "4  1.414214  1.414214\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.Feature Engineering\n",
        "> Helps improve model performance by creating meaningful new features.\n",
        "> Examples:\n",
        "*   Polynomial Features\n",
        "*   Log Transformation\n",
        "*   Feature Interaction"
      ],
      "metadata": {
        "id": "JlYS-w6nt5nI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "import numpy as np\n",
        "\n",
        "# Sample dataset\n",
        "df_features = pd.DataFrame({'x1': [1, 2, 3, 4, 5],\n",
        "                            'x2': [2, 3, 4, 5, 6]})\n",
        "\n",
        "# Generating Polynomial Features\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "df_poly = pd.DataFrame(poly.fit_transform(df_features), columns=['x1', 'x2', 'x1^2', 'x1*x2', 'x2^2'])\n",
        "\n",
        "print(\"\\nPolynomial Features:\\n\", df_poly)"
      ],
      "metadata": {
        "id": "OPpciHJ4uJ8S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}